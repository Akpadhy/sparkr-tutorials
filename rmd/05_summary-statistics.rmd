---
title: 'Computing Summary Statistics with SparkR'
author: "Sarah Armstrong, Urban Institute"
date: "July 8, 2016"
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = 'markdown')
```

**Last Updated**: May 23, 2017


**Objective**: Summary statistics and aggregations are essential means of summarizing a set of observations. In this tutorial, we discuss how to compute location, statistical dispersion, distribution and dependence measures of numerical variables in SparkR, as well as methods for examining categorical variables. In particular, we consider how to compute the following measurements and aggregations in SparkR:

_Numerical Data_

* Measures of location:
    + Mean
    + Extract summary statistics as local value
* Measures of dispersion:
    + Range width & limits
    + Variance
    + Standard deviation
    + Quantiles
* Measures of distribution shape:
    + Skewness
    + Kurtosis
* Measures of Dependence:
    + Covariance
    + Correlation

_Categorical Data_

* Frequency table
* Relative frequency table
* Contingency table

**SparkR/R Operations Discussed**: `describe`, `collect`, `showDF`, `agg`, `mean`, `typeof`, `min`, `max`, `abs`, `var`, `sd`, `skewness`, `kurtosis`, `cov`, `corr`, `count`, `n`, `groupBy`, `nrow`, `crosstab`

***

:heavy_exclamation_mark: **Warning**: Before beginning this tutorial, please visit the SparkR Tutorials README file (found [here](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/README.md)) in order to load the SparkR library and subsequently initiate a SparkR session.

```{r, include=FALSE}
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = "/home/spark")
}

library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))

sparkR.session()
```

The following error indicates that you have not initiated a SparkR session:

```{r, eval=FALSE}
Error in getSparkSession() : SparkSession not initialized
```

If you receive this message, return to the SparkR tutorials [README](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/README.md) for guidance.

***

**Read in initial data as DF**: Throughout this tutorial, we will use the loan performance example dataset that we exported at the conclusion of the [SparkR Basics I](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/01_sparkr-basics-1.md) tutorial.

```{r, message=F, warning=F, results='hide', collapse=TRUE}
df <- read.df("s3://ui-spark-social-science-public/data/hfpc_ex", 
              header = "false", 
              inferSchema = "true", 
              na.strings = "")
cache(df)
```

_Note_: documentation for the quarterly loan performance data can be found at http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html.

***


## Numerical Data

The operation `describe` (or its alias `summary`) creates a new DF that consists of several key aggregations (count, mean, max, mean, standard deviation) for a specified DF or list of DF columns (note that columns must be of a numerical datatype). We can either (1) use the action operation `showDF` to print this aggregation DF or (2) save it as a local data.frame with `collect`. Here, we perform both of these actions on the aggregation DF `sumstats_mthsremng`, which returns the aggregations listed above for the column `"mths_remng"` in `df`:

```{r, collapse=TRUE}
sumstats_mthsremng <- describe(df, "mths_remng")  # Specified list of columns here consists only of "mths_remng"

showDF(sumstats_mthsremng)  # Print the aggregation DF

sumstats_mthsremng.l <- collect(sumstats_mthsremng) # Collect aggregation DF as a local data.frame
sumstats_mthsremng.l
```

Note that measuring all five (5) of these aggregations at once can be computationally expensive with a massive data set, particularly if we are interested in only a subset of these measurements. Below, we outline ways to measure these aggregations individually, as well as several other key summary statistics for numerical data.

***


### Measures of Location


#### Mean

The mean is the only measure of central tendency currently supported by SparkR. The operations `mean` and `avg` can be used with the `agg` operation that we discussed in the [SparkR Basics II](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/02_sparkr-basics-2.md) tutorial to measure the average of a numerical DF column. Remember that `agg` returns another DF. Therefore, we can either print the DF with `showDF` or we can save the aggregation as a local data.frame. Collecting the DF may be preferred if we want to work with the mean `"mths_remng"` value as a single value in RStudio.

```{r, collapse=TRUE}
mths_remng.avg <- agg(df, mean = mean(df$mths_remng)) # Create an aggregation DF

# DataFrame
showDF(mths_remng.avg) # Print this DF
typeof(mths_remng.avg) # Aggregation DF is of class S4

# data.frame
mths_remng.avg.l <- collect(mths_remng.avg) # Collect the DF as a local data.frame
(mths_remng.avg.l <- mths_remng.avg.l[,1])  # Overwrite data.frame with numerical mean value (was entry in d.f)
typeof(mths_remng.avg.l)  # Object is now of a numerical dtype
```

***


### Measures of dispersion


#### Range width & limits

We can also use `agg` to create a DF that lists the minimum and maximum values within a numerical DF column (i.e. the limits of the range of values in the column) and the width of the range. Here, we create compute these values for `"mths_remng"` and print the resulting DF with `showDF`:

```{r, collapse=TRUE}
mr_range <- agg(df, minimum = min(df$mths_remng), maximum = max(df$mths_remng), 
                range_width = abs(max(df$mths_remng) - min(df$mths_remng)))
showDF(mr_range)
```


#### Variance & standard deviation

Again using `agg`, we compute the variance and standard deviation of `"mths_remng"` with the expressions below. Note that, here, we are computing sample variance and standard deviation (which we could also measure with their respective aliases, `variance` and `stddev`). To measure population variance and standard deviation, we would use `var_pop` and `stddev_pop`, respectively.

```{r, collapse=TRUE}
mr_var <- agg(df, variance = var(df$mths_remng))  # Sample variance
showDF(mr_var)

mr_sd <- agg(df, std_dev = sd(df$mths_remng)) # Sample standard deviation
showDF(mr_sd)
```


#### Approximate Quantiles

The operation `approxQuantile` returns approximate quantiles for a DF column. We specify the quantiles to be approximated by the operation as a vector set equal to the `probabilities` parameter, and the acceptable level of error by the `relativeError` paramter.

If the column includes `n` rows, then `approxQuantile` will return a list of quantile values with rank values that are acceptably close to those exact values specified by `probabilities`. In particular, the operation assigns approximate rank values such that the computed rank, (`probabilities * n`), falls within the inequality:


`floor((probabilities - relativeError) * n) <= rank(x) <= ceiling((probabilities + relativeError) * n)`


Below, we define a new DF, `df_`, that includes only nonmissing values for `"mths_remng"` and then compute approximate Q1, Q2 and Q3 values for `"mths_remng"`:

```{r, collapse=TRUE}
df_ <- dropna(df, cols = "mths_remng")

quartiles_mr <- approxQuantile(x = df_, col = "mths_remng", probabilities = c(0.25, 0.5, 0.75), 
                               relativeError = 0.001)
quartiles_mr
```


***


### Measures of distribution shape


#### Skewness

We can measure the magnitude and direction of skew in the distribution of a numerical DF column by using the operation `skewness` with `agg`, just as we did to measure the `mean`, `variance` and `stddev` of a numerical variable. Below, we measure the `skewness` of `"mths_remng"`:

```{r, collapse=TRUE}
mr_sk <- agg(df, skewness = skewness(df$mths_remng))
showDF(mr_sk)
```


#### Kurtosis

Similarly, we can meaure the magnitude of, and how sharp is, the central peak of the distribution of a numerical variable, i.e. the "peakedness" of the distribution, (relative to a standard bell curve) with the `kurtosis` operation. Here, we measure the `kurtosis` of `"mths_remng"`:

```{r, collapse=TRUE}
mr_kr <- agg(df, kurtosis = kurtosis(df$mths_remng))
showDF(mr_kr)
```

***


### Measures of dependence

#### Covariance & correlation

The actions `cov` and `corr` return the sample covariance and correlation measures of dependency between two DF columns, respectively. Currently, Pearson is the only supported method for calculating correlation. Here we compute the covariance and correlation of `"loan_age"` and `"mths_remng"`. Note that, in saving the covariance and correlation measures, we are not required to first `collect` locally since `cov` and `corr` return values, rather than DFs:

```{r, collapse=TRUE}
cov_la.mr <- cov(df, "loan_age", "mths_remng")
corr_la.mr <- corr(df, "loan_age", "mths_remng", method = "pearson")
cov_la.mr
corr_la.mr

typeof(cov_la.mr)
typeof(corr_la.mr)
```

***



## Categorical Data


We can compute descriptive statistics for categorical data using (1) the `groupBy` operation that we discussed in the [SparkR Basics II](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/02_sparkr-basics-2.md) tutorial and (2) operations native to SparkR for this purpose.

```{r, include=FALSE}
df$cd_zero_bal <- ifelse(isNull(df$cd_zero_bal), "Unknown", df$cd_zero_bal)
df$servicer_name <- ifelse(df$servicer_name == "", "Unknown", df$servicer_name)
```


#### Frequency table

To create a frequency table for a categorical variable in SparkR, i.e. list the number of observations for each distinct value in a column of strings, we can simply use the `count` transformation with grouped data. Group the data by the categorical variable for which we want to return a frequency table. Here, we create a frequency table for using this approach `"cd_zero_bal"`:

```{r, collapse=TRUE}
zb_f <- count(groupBy(df, "cd_zero_bal"))
showDF(zb_f)
```

We could also embed a grouping into an `agg` operation as we saw in the [SparkR Basics II](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/02_sparkr-basics-2.md) tutorial to achieve the same frequency table DF, i.e. we could evaluate the expression `agg(groupBy(df, df$cd_zero_bal), count = n(df$cd_zero_bal))`.

#### Relative frequency table

We could similarly create a DF that consists of a relative frequency table. Here, we reproduce the frequency table from the preceding section, but now including the relative frequency for each distinct string value, labeled `"Percentage"`:

```{r, collapse=TRUE}
n <- nrow(df)
zb_rf <- agg(groupBy(df, df$cd_zero_bal), Count = n(df$cd_zero_bal), Percentage = n(df$cd_zero_bal) * (100/n))
showDF(zb_rf)
```

#### Contingency table

Finally, we can create a contingency table with the operation `crosstab`, which returns a data.frame that consists of a contingency table between two categorical DF columns. Here, we create and print a contingency table for `"servicer_name"` and `"cd_zero_bal"`:

```{r, eval=FALSE}
conting_sn.zb <- crosstab(df, "servicer_name", "cd_zero_bal")
conting_sn.zb
```

Here, is the contingency table (the output of `crosstab`) in a formatted table:

```{r kable, echo=FALSE}
conting_sn.zb <- crosstab(df, "servicer_name", "cd_zero_bal")
library(knitr)
kable(conting_sn.zb)
```

__End of tutorial__ - Next up is [Merging SparkR DataFrames](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/06_merging.md)