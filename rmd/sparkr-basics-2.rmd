---
title: 'SparkR Basics II: Essential DataFrame Operations'
author: "Sarah Armstrong, Urban Institute"
date: "June 28, 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**Objective**: The SparkR DataFrame (DF) API supports a number of operations to do structured data processing. These operations range from the simple tasks that we used in the SparkR Basics I tutorial (e.g. counting the number of rows in a DF using `nrow`) to more complex tasks like computing aggregate data. This tutorial discusses the key DF operations for processing tabular data in the SparkR environment, the different types of DF operations and how to perform these operations efficiently. In particular, this tutorial:

* Aggregation, grouping
* What is a DF?
* Persistence

**SparkR/R Operations Discussed**: `read.df`, `nrow`, `ncol`, `dim`, `for`, `past0`, `rbind`, `withColumnRenamed`, `columns`, `head`, `take`, `str`, `describe`, `dtypes`, `schema`, `printSchema`, `cast`, `write.df`

***

<span style="color:red">**Warning**</span>: Before beginning this tutorial, please visit the SparkR Tutorials README file (found [here](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/README.md)) in order to load the SparkR library and subsequently initiate your SparkR and SparkR SQL contexts.

```{r, include=FALSE}
library(SparkR)
sc <- sparkR.init(sparkEnvir=list(spark.executor.memory="2g", spark.driver.memory="1g", spark.driver.maxResultSize="1g"), sparkPackages="com.databricks:spark-csv_2.11:1.4.0")
sqlContext <- sparkRSQL.init(sc)
```

You can confirm that you successfully initiated these contexts by looking at the global environment of RStudio. Only proceed if you can see `sc` and `sqlContext` listed as values in the global environment or RStudio.

***

**Read in initial data as DF**: Throughout this tutorial, we will use the loan performance example dataset that we exported at the conclusion of the SparkR Basics I tutorial.

```{r, message=F, warning=F, results='hide'}
df <- read.df(sqlContext, "s3://sparkr-tutorials/hfpc_ex", header='false', inferSchema='true')
```

```{r, include=FALSE}
cache(df)
```

_Note_: documentation for the quarterly loan performance data can be found at http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html.

***

### Aggregating & Grouping: 

Computing aagregations across a dataset is a basic goal when working with tabular data and, because our data is distributed across nodes, we must explicitly direct SparkR to perform an aggregation if we want to compute and return a summary statistic. Both the `agg` and `summarize` operations achieve this by computing aggregations of DF entries based on a specified list of columns. For example, we can return the mean loan age for all rows in the DF `df` with:

```{r, collapse=TRUE}
df1 <- agg(df, loan_age_avg = avg(df$loan_age))
showDF(df1)
```

We can compute a number of aggregations by embedding them in `agg` or `summarize`. The following list illustrates the types of summary statistics that can be computed and is not exhaustive:

* `avg`, `mean`: return the mean of a DF column
* `sd`, `stddev`, `stddev_samp`: return the unbiased sample standard deviation in the values of a DF column
* `stddev_pop`: returns the population standard deviation in a DF column
* `var`, `variance`, `var_samp`: return the unbiased variance of the values in a DF column
* `var_pop`: returns the population variance of the values in a DF column
* `countDistinct`, `n_distinct`: return the number of distinct items in a DF column
* `first`, `last`: return the first and last item in a DF column, respectively
* `max`, `min`: return the maximum and minimum of the values in a DF column
* `sum`: returns the sum of all values in a DF column

If we want to compute aggregations across the elements of a dataset that share a common identifier, we can achieve this embedding the `groupBy` operation in `agg` or `summarize`. For example, the following `agg` operation returns the mean loan age and the number of observations for each distinct `"servicer_name"` in the DataFrame `df`:

```{r, collapse=TRUE}
head(df2 <- agg(groupBy(df, df$servicer_name), loan_age_avg = avg(df$loan_age), count = n(df$loan_age)))
```

```{r, include=FALSE}
cache(df2)
```

### Arranging (Ordering) a DataFrame:

The operations `arrange` and `orderBy` allow us to sort a DF by a specified list of columns. If we want to sort the DataFrame that we just specified, `df2`, we can arrange the rows of `df2` by `"loan_age_avg"`, `"count"` or both. Note that the default for `arrange` is to order the row values as ascending:

```{r, collapse=TRUE}
head(arrange(df2, df2$loan_age_avg))
head(arrange(df2, asc(df2$loan_age_avg), asc(df2$count)))
```

We can also specify ordering as logical statements. The following expressions are equivalent to those in the preceding example:

```{r, collapse=TRUE}
head(arrange(df2, "loan_age_avg", decreasing = FALSE))
head(arrange(df2, "loan_age_avg", "count", decreasing = c(FALSE, FALSE)))
```

```{r, include=FALSE}
unpersist(df2)
```

### Append a column to a DataFrame:

There are various reasons why we might want to introduce a new column to a DataFrame. A simple example is creating a new variable within our data. In the SparkR environment, this could be acheived by appending an existing DF using the `withColumn` operation.

For example, the values of the `"loan_age"` column in `df` are the number of calendar months since the first full month that the mortgage loan accrues interest. If we want to convert the unit of time for loan age from calendar months to years and work with this measure as a variable in our analysis, we can evaluate the following `withColumn` expression:

```{r, collapse=TRUE}
head(df3 <- withColumn(df, "loan_age_yrs", df$loan_age * (1/12)))
```

Note that `df3` contains every column originally included in `df`, as well as the column `"loan_age_yrs"`.

We can also rename a DF column using the `withColumnRenamed` operation. The following expression returns a DF that is equivalent to `df`, excluding that we have renamed `"servicer_name"` to `"servicer"`.

```{r, collapse=TRUE}
head(df4 <- withColumnRenamed(df, "servicer_name", "servicer"))
```

### User-defined Functions (UDFs): [Note insert upon SparkR 2.0.0 release]

### Types of SparkR operations:

Throughout this tutorial, as well as in the SparkR Basics I tutorial, you may have noticed that some operations result in a new DataFrame (e.g. `agg`) and some return an output (e.g. `head`). SparkR operations can be classified as either:

* __transformations__: those operations that return a new SparkR DataFrame; or,
* __actions__: those operations that return an output.

A fundamental characteristic of Apache Spark that allows us SparkR-users to perform efficient analysis on massive data is that transformations are lazily evaluated, meaning that SparkR delays evaluating these operations until we direct it to return some ouput (as communicated by an action operation). We can intuitively think of transformations as instructions that SparkR acts on only once its directed to return a result.


This lazy evaluation strategy (1) reduces the number of processes SparkR is required to complete and (2) allows SparkR to interpret an entire set of instructions before acting, and then make processing decisions that are obscured from SparkR-users in order to further optimize the evaluation of the expressions that we communicate to SparkR.

### DataFrame Persistence:

Note that, in this tutorial, we have been saving the output of transformation operations (e.g. `withColumn`) in the format `dfi`. As we discussed in the preceding section, SparkR saves the output of a transformation as a SparkR DataFrame. This is distinct from an R data.frame. We store the instructions communicated by a transformation as a SparkR DataFrame. An R data.frame, conversely, is an actual data structure defined by a list of vectors.

We saved the output of the first transformation included in this tutorial, `read.df`, as `df`. This operation does not load data into SparkR. Instead, the DataFrame `df` consists of instructions that the data should be read in and how SparkR should interpret the data as it is read in. Every time we directed SparkR to evaluate the expressions

```{r, collapse=TRUE, eval=FALSE}
head(df, 5)
head(df, 10)
```

SparkR would:

1. read in the data as a DataFrame,
2. look for the first five (5) rows of the DataFrame,
3. return the first five (5) rows of the DataFrame,
4. read in the data as a DataFrame,
5. look for the first ten (10) rows of the DataFrame and
6. return the first ten (10) rows of the DataFrame.

Note that nothing is stored since the DataFrame is not data! This would be incredibly inefficient if not for the `cache` operation, which directs each node in our cluster to store in memory any partitions of a DataFrame that it computes (in the course of evaluating an action) and then to reuse them in subsequent actions evaluated on that DataFrame (or DataFrames derived from it).


By caching a given DataFrame, we can ensure that future actions on that DataFrame (or those derived from it) are evaluated much more efficiently. Both `cache` and `persist` can be used to cache a DataFrame. The `cache` operation stores a DataFrame in memory, while `persist` allows SparkR-users to persist a DataFrame using different storage levels (i.e. store to disk, memory or both). The default storage level for `persist` is memory only and, at this storage level, `persist` and `cache` are equivalent operations. More often than not, we can simply use `cache`: if our DataFrames can fit in memory only, then we should exclusively store DataFrames in memory only since this is the most CPU-efficient storage option.


Now that we have some understanding of how DataFrame persistence works in SparkR, let's see how this operation affects the processes in the preceding example. By including `cache` in our expressions as

```{r, eval=FALSE, collapse=TRUE}
df_ <- read.df(sqlContext, "s3://sparkr-tutorials/hfpc_ex", header='false', inferSchema='true')
cache(df_)
head(df_, 5)
head(df_, 10)
```

The steps performed by SparkR change to:

1. read in the data as a DataFrame,
2. look for the first five (5) rows of the DataFrame,
3. return the first five (5) rows of the DataFrame,
4. cache the DataFrame
5. look for the first ten (10) rows of the DataFrame (using the cache) and
6. return the first ten (10) rows of the DataFrame.

While the number of steps required remains six (6), the time required to `cache` a DataFrame once is significantly less than that required to read in data as a DataFrame several times. If we continuited to perform actions on `df_`, clearly directing SparkR to cache the DataFrame would reduce our overal evaluation time. We can direct SparkR to stop persisting a DataFrame with the `unpersist` operation:

```{r, eval=FALSE}
unpersist(df_)
```

We can compare computation time for several sequences of operations with, and without, caching:

```{r, collapse=TRUE}
.df <- read.df(sqlContext, "s3://sparkr-tutorials/hfpc_ex", header='false', inferSchema='true')
(t1 <- system.time(ncol(.df)))
(t2 <- system.time(nrow(.df)))
(t3 <- system.time(dim(.df)))
rm(.df)

.df <- read.df(sqlContext, "s3://sparkr-tutorials/hfpc_ex", header='false', inferSchema='true')
cache(.df)
(t1_ <- system.time(ncol(.df)))
(t2_ <- system.time(nrow(.df)))
(t3_ <- system.time(dim(.df)))
unpersist(.df)
rm(.df)
```

__End of tutorial__ - Next up is [Insert next tutorial]