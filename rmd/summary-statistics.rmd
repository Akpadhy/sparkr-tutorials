---
title: 'Summary Statistics'
author: "Sarah Armstrong, Urban Institute"
date: "July 8, 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**Objective**: Summary statistics and aggregations are essential means of summarizing a set of observations. In this tutorial, we discuss how to compute location, statistical dispersion, distribution and dependence measures of numerical variables in SparkR, as well as methods for examining categorical variables. In particular, we consider how to compute the following measurements in SparkR:

_Numerical Data_

* Measures of location:
    + Mean
    + Extract summary statistics as local value
* Measures of dispersion:
    + Range width & limits
    + Variance
    + Standard deviation
    + Quantiles
* Measures of distribution shape:
    + Skewness
    + Kurtosis
* Measures of Dependence:
    + Covariance
    + Correlation

_Categorical Data_

* Frequency table
* Relative frequency table
* Contingency table

**SparkR/R Operations Discussed**: `describe`, `collect`, `showDF`, `agg`, `mean`, `typeof`, `min`, `max`, `abs`, `var`, `sd`, `skewness`, `kurtosis`, `cov`, `corr`, `count`, `n`, `groupBy`, `nrow`, `crosstab`

***

<span style="color:red">**Warning**</span>: Before beginning this tutorial, please visit the SparkR Tutorials README file (found [here](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/README.md)) in order to load the SparkR library and subsequently initiate your SparkR and SparkR SQL contexts.

```{r, include=FALSE}
library(SparkR)
sc <- sparkR.init(sparkEnvir=list(spark.executor.memory="2g", spark.driver.memory="1g", spark.driver.maxResultSize="1g"), sparkPackages="com.databricks:spark-csv_2.11:1.4.0")
sqlContext <- sparkRSQL.init(sc)
```

You can confirm that you successfully initiated these contexts by looking at the global environment of RStudio. Only proceed if you can see `sc` and `sqlContext` listed as values in the global environment or RStudio.

***

**Read in initial data as DF**: Throughout this tutorial, we will use the loan performance example dataset that we exported at the conclusion of the SparkR Basics I tutorial.

```{r, message=F, warning=F, results='hide', collapse=TRUE}
df <- read.df(sqlContext, "s3://sparkr-tutorials/hfpc_ex", header='false', inferSchema='true')
cache(df)
```

_Note_: documentation for the quarterly loan performance data can be found at http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html.

***


## Numerical Data

The operation `describe` (or its alias `summary`) creates a new DF that consists of several key aggregations (count, mean, max, mean, standard deviation) for a specified DF or list of DF columns (note that columns must be of a numerical datatype). We can either (1) use the action operation `showDF` to print this aggregation DF or (2) save it as a local data.frame with `collect`. Here, we perform both of these actions on the aggregation DF `sumstats_mthsremng`, which returns the aggregations listed above for the column `"mths_remng"` in `df`:

```{r, collapse=TRUE}
sumstats_mthsremng <- describe(df, "mths_remng")  # Specified list of columns here consists only of "mths_remng"

showDF(sumstats_mthsremng)  # Print the aggregation DF

sumstats_mthsremng.l <- collect(sumstats_mthsremng) # Collect aggregation DF as a local data.frame
sumstats_mthsremng.l
```

Note that measuring all five (5) of these aggregations at once is computationally expensive, particularly if we are interested in only a subset of these measurements. Below, we outline ways to measure these aggregations individually, as well as several other key summary statistics for numerical data.

***


### Measures of Location:


#### Mean

While there are several measures of central tendency, SparkR currently only supports computing averages of numerical DF columns. The operations `mean` and `avg` can be used with the `agg` operation that we discussed in the SparkR Basics II tutorial to measure the average of a numerical DF column. Remember that `agg` returns another DF. Therefore, we can either print the DF with `showDF` or we can save the aggregation as a local data.frame. Collecting the DF may be preferred if we want to work with the mean `"mths_remng"` value as a single value in RStudio.

```{r, collapse=TRUE}
mths_remng.avg <- agg(df, mean = mean(df$mths_remng)) # Create an aggregation DF
showDF(mths_remng.avg) # Print this DF
typeof(mths_remng.avg) # Aggregation DF is of class S4

mths_remng.avg.l <- collect(mths_remng.avg) # Collect the DF as a local data.frame
(mths_remng.avg.l <- mths_remng.avg.l[,1])  # Overwrite data.frame with numerical mean value (was entry in d.f)
typeof(mths_remng.avg.l)  # Object is now of a numerical dtype
```

***


### Measures of dispersion:


#### Range width & limits:

We can also use `agg` to create a DF that lists the minimum and maximum values within a numerical DF column (i.e. the limits of the range of values in the column) and the width of the range of these values. Here, we create compute these values for `"mths_remng"` and print the resulting DF with `showDF`:

```{r, collapse=TRUE}
mr_range <- agg(df, minimum = min(df$mths_remng), maximum = max(df$mths_remng), range_width = abs(max(df$mths_remng) - min(df$mths_remng)))
showDF(mr_range)
```


#### Variance & standard deviation:

Again using `agg`, we compute the variance and standard deviation of `"mths_remng"` with the expressions below. Note that, here, we are computing sample variance and standard deviation (which we could also measure with their respective aliases, `variance` and `stddev`). To measure population variance and standard deviation, we would use `var_pop` and `stddev_pop`, respectively.

```{r, collapse=TRUE}
mr_var <- agg(df, variance = var(df$mths_remng))  # Sample variance
showDF(mr_var)

mr_sd <- agg(df, std_dev = sd(df$mths_remng)) # Sample standard deviation
showDF(mr_sd)
```


#### Quantiles:

[Insert section on measuring (approx. quantiles) with release of SparkR 2.0.0]


***


### Measures of distribution shape:


#### Skewness:

We can measure the magnitude and direction of skew in the distribution of a numerical column (relative to horizontal symmetry) in a DF by using the operation `skewness` with `agg`, just as we did to measure the `mean`, `variance` and `stddev` of a numerical variable. Below, we measure the `skewness` of `"mths_remng"`:

```{r, collapse=TRUE}
mr_sk <- agg(df, skewness = skewness(df$mths_remng))
showDF(mr_sk)
```


#### Kurtosis:

Similarly, we can meaure the magnitude of, and how sharp is, the central peak of the distribution of a numerical variable, i.e. the "peakedness" of the distribution, (relative to a standard bell curve) with the `kurtosis` operation. Here, we measure the `kurtosis` of `"mths_remng"`:

```{r, collapse=TRUE}
mr_kr <- agg(df, kurtosis = kurtosis(df$mths_remng))
showDF(mr_kr)
```

***


### Measures of dependence:

#### Covariance & correlation:

The actions `cov` and `corr` return the sample covariance and correlation measures of dependency between two DF columns, respectively. Currently, Pearson is the only supported method for calculating correlation. Here we compute the covariance and correlation of `"loan_age"` and `"mths_remng"`. Note that, in saving the covariance and correlation measures, we are not required to first `collect` locally since `cov` and `corr` return values, rather than DFs:

```{r, collapse=TRUE}
cov_la.mr <- cov(df, "loan_age", "mths_remng")
corr_la.mr <- corr(df, "loan_age", "mths_remng", method = "pearson")
cov_la.mr
corr_la.mr

typeof(cov_la.mr)
typeof(corr_la.mr)
```

***


## Categorical Data


We can compute descriptive statistics for categorical data using the `groupBy` operation that we used in the Basics II tutorial to compute aggregations of numerical data over groups, as well as operations native to SparkR for this purpose.

```{r, include=FALSE}
df$cd_zero_bal <- ifelse(isNull(df$cd_zero_bal), "Unknown", df$cd_zero_bal)
df$servicer_name <- ifelse(df$servicer_name == "", "Unknown", df$servicer_name)
```

***


#### Frequency table:

To create a frequency table for a categorical variable in SparkR, i.e. list the number of observations for each distinct value in a column of strings, we can simply use the `count` transformation with grouped data. Group the data by the categorical variable for which we want to return a frequency table. Here, we create a frequency table for using this approach `"cd_zero_bal"`:

```{r, collapse=TRUE}
zb_f <- count(groupBy(df, "cd_zero_bal"))
showDF(zb_f)
```

We could also embed a grouping into an `agg` operation as we saw in the Basics II tutorial to achieve the same frequency table DF, i.e. we could evaluate the expression `agg(groupBy(df, df$cd_zero_bal), count = n(df$cd_zero_bal))`.

#### Relative frequency table:

We could similarly create a DF that consists of a relative frequency table. Here, we reproduce the frequency table from the preceding section, but now including the relative frequency for each distinct string value, as measured by "Percentage":

```{r, collapse=TRUE}
n <- nrow(df)
zb_rf <- agg(groupBy(df, df$cd_zero_bal), Count = n(df$cd_zero_bal), Percentage = n(df$cd_zero_bal) * (100/n))
showDF(zb_rf)
```

#### Contingency table:

Finally, we can create a contingency table with the operation `crosstab`, which returns a data.frame that consists of a contingency table between two categorical columns of a DF. Here, we create and print a contingency table for `"servicer_name"` and `"cd_zero_bal"`:

```{r, eval=FALSE}
conting_sn.zb <- crosstab(df, "servicer_name", "cd_zero_bal")
conting_sn.zb
```

Here, is the contingency table, the output of `crosstab`, in a formatted table:

```{r kable, echo=FALSE}
conting_sn.zb <- crosstab(df, "servicer_name", "cd_zero_bal")
library(knitr)
kable(conting_sn.zb)
```

__End of tutorial__ - Next up is [Insert next tutorial]
